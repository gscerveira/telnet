{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7T4DFQwvSMvL",
        "outputId": "0ff6d211-27e1-4bb5-f988-fb512027b3df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "TELNET_DATADIR set to: /content/drive/MyDrive/telnet_data\n",
            "/content\n",
            "Cloning into 'telnet'...\n",
            "remote: Enumerating objects: 345, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 345 (delta 27), reused 40 (delta 17), pack-reused 295 (from 1)\u001b[K\n",
            "Receiving objects: 100% (345/345), 21.48 MiB | 49.88 MiB/s, done.\n",
            "Resolving deltas: 100% (176/176), done.\n",
            "/content/telnet\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 - Setup: Clone repo and install dependencies\n",
        "# Mount Google Drive for output storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# Create output directory on Drive\n",
        "TELNET_DATADIR = '/content/drive/MyDrive/telnet_data'\n",
        "os.makedirs(TELNET_DATADIR, exist_ok=True)\n",
        "os.makedirs(f'{TELNET_DATADIR}/data/models', exist_ok=True)\n",
        "os.makedirs(f'{TELNET_DATADIR}/shapefiles', exist_ok=True)\n",
        "os.makedirs(f'{TELNET_DATADIR}/results', exist_ok=True)\n",
        "os.environ['TELNET_DATADIR'] = TELNET_DATADIR\n",
        "print(f\"TELNET_DATADIR set to: {TELNET_DATADIR}\")\n",
        "\n",
        "# Clone the repo\n",
        "%cd /content\n",
        "!rm -rf telnet\n",
        "!git clone https://github.com/gscerveira/telnet.git\n",
        "%cd telnet\n",
        "\n",
        "# Install uv for fast package management\n",
        "!pip install -q uv\n",
        "\n",
        "# Install dependencies\n",
        "!uv pip install --system -q -r docker/requirements.txt\n",
        "!uv pip install --system -q gcsfs s3fs geopandas rioxarray\n",
        "\n",
        "print(\"\\nDependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1.5 - Hardcode Maranhão coordinates (for non-interactive runs)\n",
        "# Creates lat_lon_boundaries.txt so the notebook can run without user input\n",
        "\n",
        "import os\n",
        "\n",
        "# Maranhão state bounding box coordinates\n",
        "MARANHAO_COORDS = {\n",
        "    'init_lat': -10.5,   # Southernmost latitude\n",
        "    'final_lat': -1.0,   # Northernmost latitude\n",
        "    'init_lon': -49.0,   # Westernmost longitude\n",
        "    'final_lon': -41.5,  # Easternmost longitude\n",
        "}\n",
        "\n",
        "# Create the boundaries file in the repo data directory\n",
        "data_dir = '/content/telnet/data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "bounds_file = os.path.join(data_dir, 'lat_lon_boundaries.txt')\n",
        "with open(bounds_file, 'w') as f:\n",
        "    f.write(f\"{MARANHAO_COORDS['init_lat']}\\n\")\n",
        "    f.write(f\"{MARANHAO_COORDS['final_lat']}\\n\")\n",
        "    f.write(f\"{MARANHAO_COORDS['init_lon']}\\n\")\n",
        "    f.write(f\"{MARANHAO_COORDS['final_lon']}\\n\")\n",
        "\n",
        "print(f\"Maranhão coordinates hardcoded to: {bounds_file}\")\n",
        "print(f\"  Latitude: {MARANHAO_COORDS['init_lat']} to {MARANHAO_COORDS['final_lat']}\")\n",
        "print(f\"  Longitude: {MARANHAO_COORDS['init_lon']} to {MARANHAO_COORDS['final_lon']}\")"
      ],
      "metadata": {
        "id": "ssgIXO_BSMvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R716wydUSMvO"
      },
      "outputs": [],
      "source": [
        "# Cell 2 - Verify GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU available! Switch to a GPU runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsiD7W9oSMvP"
      },
      "outputs": [],
      "source": [
        "# Cell 3 - Test ARCO ERA5 Access (quick connectivity check)\n",
        "%cd /content/telnet\n",
        "\n",
        "print(\"Testing ARCO ERA5 access...\")\n",
        "import gcsfs\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(token='anon')\n",
        "path = 'gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
        "files = fs.ls(path)[:5]\n",
        "print(\"Connection successful! Found files:\")\n",
        "for f in files:\n",
        "    print(f\"  {f}\")\n",
        "print(\"\\nARCO ERA5 is accessible.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.5 - Setup CDS API credentials for ERA5 download\n",
        "# You need a free account at https://cds.climate.copernicus.eu/\n",
        "# Get your API key from: https://cds.climate.copernicus.eu/how-to-api\n",
        "\n",
        "import os\n",
        "\n",
        "# INSTRUCTIONS:\n",
        "# 1. Register for a free account at https://cds.climate.copernicus.eu/\n",
        "# 2. Go to https://cds.climate.copernicus.eu/how-to-api to get your API key\n",
        "# 3. Replace 'YOUR-API-KEY-HERE' with your actual API key\n",
        "\n",
        "CDS_URL = \"https://cds.climate.copernicus.eu/api\"\n",
        "CDS_KEY = \"YOUR-API-KEY-HERE\"  # <-- Replace with your API key from CDS website\n",
        "\n",
        "# Create .cdsapirc file\n",
        "cdsapirc_content = f\"\"\"url: {CDS_URL}\n",
        "key: {CDS_KEY}\n",
        "\"\"\"\n",
        "\n",
        "cdsapirc_path = os.path.expanduser('~/.cdsapirc')\n",
        "with open(cdsapirc_path, 'w') as f:\n",
        "    f.write(cdsapirc_content)\n",
        "\n",
        "print(\"CDS API credentials configured.\")\n",
        "print(f\"Config file written to: {cdsapirc_path}\")\n",
        "print()\n",
        "if CDS_KEY == \"YOUR-API-KEY-HERE\":\n",
        "    print(\"⚠️  WARNING: You need to replace 'YOUR-API-KEY-HERE' with your actual API key!\")\n",
        "    print(\"   Register at: https://cds.climate.copernicus.eu/\")\n",
        "    print(\"   Get API key: https://cds.climate.copernicus.eu/how-to-api\")\n",
        "else:\n",
        "    print(\"✓ API key configured. Ready to download ERA5 data.\")"
      ],
      "metadata": {
        "id": "GV90TGwYSMvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dSubbj4SMvQ"
      },
      "outputs": [],
      "source": [
        "# Cell 4 - Download ERSSTv5 and Maranhao shapefile\n",
        "%cd /content/telnet\n",
        "import os\n",
        "os.environ['TELNET_DATADIR'] = '/content/drive/MyDrive/telnet_data'\n",
        "\n",
        "# Download ERSSTv5\n",
        "print(\"=\" * 60)\n",
        "print(\"  Downloading ERSSTv5 sea surface temperature data...\")\n",
        "print(\"=\" * 60)\n",
        "from download_preprocess_data import download_ersstv5\n",
        "download_ersstv5('1940-01-01', '2025-12-01')\n",
        "\n",
        "# Download shapefile\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"  Downloading Maranhao shapefile...\")\n",
        "print(\"=\" * 60)\n",
        "!python download_maranhao_shapefile.py\n",
        "\n",
        "print(\"\\nDownloads complete!\")\n",
        "print(\"\\nNOTE: For ERA5 data, download manually from CDS:\")\n",
        "print(\"  - https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means\")\n",
        "print(\"  - https://cds.climate.copernicus.eu/datasets/reanalysis-era5-pressure-levels-monthly-means\")\n",
        "print(\"Then run Cell 4.5 to preprocess.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4.5 - Preprocess ERA5 files for climate indices\n",
        "# Run this after downloading ERA5 data from CDS\n",
        "# This creates the preprocessed files needed by compute_climate_indices.py\n",
        "\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "era5_dir = '/content/drive/MyDrive/telnet_data/era5'\n",
        "os.makedirs(era5_dir, exist_ok=True)\n",
        "\n",
        "# Check what raw files exist\n",
        "print(\"Checking for ERA5 raw files...\")\n",
        "required_files = {\n",
        "    'u10': 'era5_u10_1940-2025.nc',\n",
        "    'v10': 'era5_v10_1940-2025.nc',\n",
        "    'hgt_500': 'era5_hgt_500_1940-2025.nc',\n",
        "    'hgt_700': 'era5_hgt_700_1940-2025.nc',\n",
        "    'hgt_1000': 'era5_hgt_1000_1940-2025.nc',\n",
        "}\n",
        "\n",
        "missing = []\n",
        "for var, fname in required_files.items():\n",
        "    fpath = os.path.join(era5_dir, fname)\n",
        "    if os.path.exists(fpath):\n",
        "        size = os.path.getsize(fpath) / 1e9\n",
        "        print(f\"  ✓ {fname} ({size:.2f} GB)\")\n",
        "    else:\n",
        "        print(f\"  ✗ {fname} MISSING\")\n",
        "        missing.append(var)\n",
        "\n",
        "if missing:\n",
        "    print(f\"\\nMissing files: {missing}\")\n",
        "    print(\"Download from CDS and place in:\", era5_dir)\n",
        "    raise FileNotFoundError(\"Missing ERA5 raw files - see above\")\n",
        "\n",
        "# Target 2-degree grid (global)\n",
        "lat2interp = np.arange(-88., 90., 2.0)[::-1]\n",
        "lon2interp = np.arange(0., 360., 2.0)\n",
        "\n",
        "# Process u10\n",
        "print(\"\\nProcessing u10...\")\n",
        "ds = xr.open_dataset(f'{era5_dir}/era5_u10_1940-2025.nc')\n",
        "ds = ds.interp(latitude=lat2interp, longitude=lon2interp, method='linear')\n",
        "ds = ds.rename({'valid_time': 'time', 'latitude': 'lat', 'longitude': 'lon'})\n",
        "ds.to_netcdf(f'{era5_dir}/era5_u10_1940-present_preprocessed.nc')\n",
        "print(\"  Saved era5_u10_1940-present_preprocessed.nc\")\n",
        "ds.close()\n",
        "\n",
        "# Process v10\n",
        "print(\"Processing v10...\")\n",
        "ds = xr.open_dataset(f'{era5_dir}/era5_v10_1940-2025.nc')\n",
        "ds = ds.interp(latitude=lat2interp, longitude=lon2interp, method='linear')\n",
        "ds = ds.rename({'valid_time': 'time', 'latitude': 'lat', 'longitude': 'lon'})\n",
        "ds.to_netcdf(f'{era5_dir}/era5_v10_1940-present_preprocessed.nc')\n",
        "print(\"  Saved era5_v10_1940-present_preprocessed.nc\")\n",
        "ds.close()\n",
        "\n",
        "# Process geopotential (combine 3 levels, convert to height)\n",
        "print(\"Processing geopotential height...\")\n",
        "ds_500 = xr.open_dataset(f'{era5_dir}/era5_hgt_500_1940-2025.nc')\n",
        "ds_700 = xr.open_dataset(f'{era5_dir}/era5_hgt_700_1940-2025.nc')\n",
        "ds_1000 = xr.open_dataset(f'{era5_dir}/era5_hgt_1000_1940-2025.nc')\n",
        "\n",
        "# Convert geopotential to height (divide by g=9.80665)\n",
        "g = 9.80665\n",
        "ds_500['height'] = ds_500['z'] / g\n",
        "ds_700['height'] = ds_700['z'] / g\n",
        "ds_1000['height'] = ds_1000['z'] / g\n",
        "\n",
        "# Combine levels\n",
        "ds = xr.concat([ds_500['height'], ds_700['height'], ds_1000['height']], dim='pressure_level')\n",
        "ds = ds.assign_coords(pressure_level=[500, 700, 1000])\n",
        "ds = ds.to_dataset(name='height')\n",
        "\n",
        "ds = ds.interp(latitude=lat2interp, longitude=lon2interp, method='linear')\n",
        "ds = ds.rename({'valid_time': 'time', 'latitude': 'lat', 'longitude': 'lon'})\n",
        "ds.to_netcdf(f'{era5_dir}/era5_hgt_1940-present_preprocessed.nc')\n",
        "print(\"  Saved era5_hgt_1940-present_preprocessed.nc\")\n",
        "\n",
        "ds_500.close()\n",
        "ds_700.close()\n",
        "ds_1000.close()\n",
        "\n",
        "print(\"\\nDone! Ready to run climate indices (Cell 5).\")"
      ],
      "metadata": {
        "id": "iehWFjAWSMvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy_dWc7fSMvT"
      },
      "outputs": [],
      "source": [
        "# Cell 5 - Compute Climate Indices (from ERSSTv5 data)\n",
        "%cd /content/telnet\n",
        "import os\n",
        "os.environ['TELNET_DATADIR'] = '/content/drive/MyDrive/telnet_data'\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"  Computing climate indices from ERSSTv5...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use 202412 to generate seasonal_climate_indices_1941-2024.txt\n",
        "# (202512 would create a different filename that other scripts don't expect)\n",
        "!python compute_climate_indices.py --finaldate 202412\n",
        "\n",
        "print(\"\\nClimate indices computed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG_EILnJSMvU"
      },
      "outputs": [],
      "source": [
        "# Cell 6 - Feature Pre-Selection (PMI ranking)\n",
        "# Uses local ERA5 precipitation data (preprocessed from CDS)\n",
        "%cd /content/telnet\n",
        "import os\n",
        "os.environ['TELNET_DATADIR'] = '/content/drive/MyDrive/telnet_data'\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"  Running feature pre-selection (PMI ranking)\")\n",
        "print(\"  Using local ERA5 precipitation data...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use fewer samples for faster testing (100 is good, 1000 for production)\n",
        "N_SAMPLES = 100\n",
        "\n",
        "!python feature_pre_selection.py -n {N_SAMPLES}\n",
        "\n",
        "# Copy results to Drive\n",
        "import shutil\n",
        "src = '/content/telnet/data/models/final_feats.txt'\n",
        "dst = '/content/drive/MyDrive/telnet_data/data/models/final_feats.txt'\n",
        "if os.path.exists(src):\n",
        "    shutil.copy(src, dst)\n",
        "    print(f\"Copied final_feats.txt to Drive\")\n",
        "\n",
        "src_seeds = '/content/telnet/data/seeds_pmi.txt'\n",
        "dst_seeds = '/content/drive/MyDrive/telnet_data/data/seeds_pmi.txt'\n",
        "if os.path.exists(src_seeds):\n",
        "    shutil.copy(src_seeds, dst_seeds)\n",
        "    print(f\"Copied seeds_pmi.txt to Drive\")\n",
        "\n",
        "print(\"\\nFeature pre-selection complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEMw4eAcSMvV"
      },
      "outputs": [],
      "source": [
        "# Cell 7 - Model Selection (GPU grid search)\n",
        "%cd /content/telnet\n",
        "import os\n",
        "os.environ['TELNET_DATADIR'] = '/content/drive/MyDrive/telnet_data'\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"  Running model selection (hyperparameter grid search)\")\n",
        "print(\"  This is GPU-intensive and will take 2-3 hours...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!chmod +x model_selection.sh\n",
        "!./model_selection.sh 100 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvbaqfK3SMvW"
      },
      "outputs": [],
      "source": [
        "# Cell 8 - Model Testing (final training with best hyperparameters)\n",
        "%cd /content/telnet\n",
        "import os\n",
        "os.environ['TELNET_DATADIR'] = '/content/drive/MyDrive/telnet_data'\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"  Running model testing (final training)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!python model_testing.py -n 100 -c 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ImNL-tVSMvX"
      },
      "outputs": [],
      "source": [
        "# Cell 9 - Generate Forecasts (4 quarterly initializations)\n",
        "%cd /content/telnet\n",
        "import os\n",
        "os.environ['TELNET_DATADIR'] = '/content/drive/MyDrive/telnet_data'\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"  Generating seasonal forecasts...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "!chmod +x generate_forecast.sh\n",
        "\n",
        "init_dates = ['202501', '202504', '202507', '202510']\n",
        "\n",
        "for init_date in init_dates:\n",
        "    print(f\"\\n=== Forecast for {init_date} ===\")\n",
        "    !./generate_forecast.sh {init_date} 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0SiAdZGSMvX"
      },
      "outputs": [],
      "source": [
        "# Cell 10 - Extract Maranhao Region from forecasts\n",
        "%cd /content/telnet\n",
        "import os\n",
        "import glob\n",
        "\n",
        "os.environ['TELNET_DATADIR'] = '/content/drive/MyDrive/telnet_data'\n",
        "DATADIR = os.environ['TELNET_DATADIR']\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"  Extracting Maranhao region from forecasts...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "init_dates = ['202501', '202504', '202507', '202510']\n",
        "\n",
        "for init_date in init_dates:\n",
        "    results_dir = f'{DATADIR}/results/{init_date}'\n",
        "    if os.path.exists(results_dir):\n",
        "        for f in glob.glob(f'{results_dir}/*.nc'):\n",
        "            basename = os.path.basename(f)\n",
        "            if not basename.startswith('maranhao_'):\n",
        "                output = f'{results_dir}/maranhao_{basename}'\n",
        "                !python extract_maranhao.py \"{f}\" \"{output}\" --shapefile-dir {DATADIR}/shapefiles\n",
        "                print(f\"Extracted: {output}\")\n",
        "\n",
        "print(\"\\nMaranhao extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Juwl0mpSMvX"
      },
      "outputs": [],
      "source": [
        "# Cell 11 - View Results Summary\n",
        "import os\n",
        "import glob\n",
        "\n",
        "DATADIR = '/content/drive/MyDrive/telnet_data'\n",
        "init_dates = ['202501', '202504', '202507', '202510']\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"  ARCO ERA5 Workflow Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"Results saved to Google Drive:\")\n",
        "print(f\"  {DATADIR}/results/\")\n",
        "print()\n",
        "\n",
        "for init_date in init_dates:\n",
        "    results_dir = f'{DATADIR}/results/{init_date}'\n",
        "    if os.path.exists(results_dir):\n",
        "        files = os.listdir(results_dir)\n",
        "        nc_files = [f for f in files if f.endswith('.nc')]\n",
        "        print(f\"{init_date}: {len(nc_files)} forecast files\")\n",
        "        for f in sorted(nc_files)[:3]:\n",
        "            print(f\"  - {f}\")\n",
        "        if len(nc_files) > 3:\n",
        "            print(f\"  ... and {len(nc_files) - 3} more\")\n",
        "    else:\n",
        "        print(f\"{init_date}: No results yet\")\n",
        "\n",
        "print()\n",
        "print(\"Feature ranking saved to:\")\n",
        "print(f\"  {DATADIR}/data/models/final_feats.txt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}